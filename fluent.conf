<source>
  type tail

  #path- where you placed your input data
  path /fluentd/input/Customer.csv

  # pos_file where you record file position
  pos_file /fluentd/log/customer.log.pos

  #fluentd tag
  tag bo.rest.customer.insert

  #input file format
  format csv

  # keys - columns in csv file
  keys id,name,language,state,company,gender,age 
  
  #types - string/bool/integer/float/time/array
  types age:integer

</source>

<source>
  type tail

  #path- where you placed your input data
  path /fluentd/input/Customer_binary.csv

  # pos_file where you record file position
  pos_file /fluentd/log/customer_binary.log.pos

  #fluentd tag
  tag bo.insert_avro.customer

  #input file format
  format csv

  # keys - columns in csv file
  keys id,name,language,state,company,gender,age

  #types - string/bool/integer/float/time/array
  types age:integer

</source>

# send data to BigObject using avro by providing schema_file in each table
<match bo.insert_avro.*>
  type bigobject_avro

  log_level debug 

  # specify the bigobject_url to connect to
  bigobject_hostname 192.168.59.103
  bigobject_port 9091

  remove_tag_prefix bo.insert_avro. 
  flush_interval 60s

  <table>
      pattern customer
      schema_file /fluentd/input/avsc/Customer_binary.avsc
  </table>
</match>

# send data to BigObject using Restful API. Tables need to be created in advance in BigObject.
<match bo.rest.**>
  type bigobject

  log_level debug 

  # specify the bigobject_url to connect to
  bigobject_hostname 192.168.59.103
  bigobject_port 9090

  remove_tag_prefix bo.rest. 
  flush_interval 60s

  <table>
      table Customer 
      pattern customer
      
      #optional-
      #column_mapping id,name,language,state,company,gender,age
      #bo_workspace
      #bo_opts
  </table>
</match>

## match tag=debug.** and dump to console
<match debug.**>
  @type stdout
  @id stdout_output
</match>

## match not matched logs and write to file
<match **>
  @type file
  path ./log/else
  compress gz
</match>
